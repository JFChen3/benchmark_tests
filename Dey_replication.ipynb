{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicating the Results of Dey *et al.* 2014\n",
    "This notebook describes the creation of machine learning models for the band gaps for chalcopyrite compounds, following the paper by [Dey *et al.* (2014)](http://dx.doi.org/10.1016/j.commatsci.2013.10.016). In particular, recreates the hyper-parameter tuning, model evaluation, and feature selection reported in this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LassoLars\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, LeaveOneOut\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data\n",
    "\n",
    "The data for this notebook was extracted from Tables 1 and 2 from the original paper. These are stored as `csv` files in the same directory as this notebook, for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load element data file into a Pandas Dataframe\n",
    "`element_data.csv` contains elemental properties for the all possible elements in the search space, which was provided in Table 1 of the paper.\n",
    "\n",
    "Specifically, this file contains:\n",
    "- EN: Electronegativity\n",
    "- AN: Atomic Number\n",
    "- MP: Melting point (K)\n",
    "- PR: Pseudopotential radius (as defined by [Suh and Rajan (2004)](http://www.sciencedirect.com/science/article/pii/S0169433203009188?via%3Dihub))\n",
    "- VL: Valency (as defined by as given by [Suh and Rajan (2004)](http://www.sciencedirect.com/science/article/pii/S0169433203009188?via%3Dihub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in lookup tables: Ag Al As Cd Cu Ga Ge In P S Se Si Sn Te Zn\n"
     ]
    }
   ],
   "source": [
    "#Load input data into dictionary\n",
    "elem_data = pd.read_csv(\"datasets/dey_element_data.csv\")\n",
    "elem_properties = elem_data.set_index(\"#ELM\").T.to_dict(\"list\")\n",
    "print 'Elements in lookup tables:', ' '.join(sorted(elem_properties.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data\n",
    "`training_set.csv` contains experimentally determined band gap values for each compound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>composition</th>\n",
       "      <th>band_gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CuAlS2</td>\n",
       "      <td>3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CuAlSe2</td>\n",
       "      <td>2.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CuAlTe2</td>\n",
       "      <td>2.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CuGaS2</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CuGaSe2</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CuGaTe2</td>\n",
       "      <td>1.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CuInS2</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CuInSe2</td>\n",
       "      <td>1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CuInTe2</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AgAlS2</td>\n",
       "      <td>3.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AgAlSe2</td>\n",
       "      <td>2.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AgAlTe2</td>\n",
       "      <td>2.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AgGaS2</td>\n",
       "      <td>2.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AgGaSe2</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AgGaTe2</td>\n",
       "      <td>1.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AgInS2</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AgInSe2</td>\n",
       "      <td>1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AgInTe2</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ZnSiP2</td>\n",
       "      <td>2.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ZnSiAs2</td>\n",
       "      <td>1.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ZnGeP2</td>\n",
       "      <td>2.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ZnGeAs2</td>\n",
       "      <td>1.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CdSiP2</td>\n",
       "      <td>2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>CdSiAs2</td>\n",
       "      <td>1.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>CdGeP2</td>\n",
       "      <td>1.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>CdGeAs2</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>CdSnP2</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>CdSnAs2</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   composition  band_gap\n",
       "0       CuAlS2      3.49\n",
       "1      CuAlSe2      2.67\n",
       "2      CuAlTe2      2.06\n",
       "3       CuGaS2      2.43\n",
       "4      CuGaSe2      1.68\n",
       "5      CuGaTe2      1.12\n",
       "6       CuInS2      1.53\n",
       "7      CuInSe2      1.04\n",
       "8      CuInTe2      1.06\n",
       "9       AgAlS2      3.13\n",
       "10     AgAlSe2      2.55\n",
       "11     AgAlTe2      2.27\n",
       "12      AgGaS2      2.64\n",
       "13     AgGaSe2      1.80\n",
       "14     AgGaTe2      1.32\n",
       "15      AgInS2      1.87\n",
       "16     AgInSe2      1.24\n",
       "17     AgInTe2      0.95\n",
       "18      ZnSiP2      2.07\n",
       "19     ZnSiAs2      1.74\n",
       "20      ZnGeP2      2.05\n",
       "21     ZnGeAs2      1.15\n",
       "22      CdSiP2      2.33\n",
       "23     CdSiAs2      1.55\n",
       "24      CdGeP2      1.72\n",
       "25     CdGeAs2      0.57\n",
       "26      CdSnP2      1.17\n",
       "27     CdSnAs2      0.26"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_file = \"datasets/dey_training_set.csv\"\n",
    "training_set = pd.read_csv(\"datasets/dey_training_set.csv\")\n",
    "training_set.columns=[\"composition\", \"band_gap\"]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Attributes\n",
    "Attributes are what serve as input in to the machine learning model. In this section, we compute attributes given the composition of the materials.\n",
    "\n",
    "All of the compounds in the training set are $ABC_2$ compounds where $A$ are Cu-group elements, Zn-group elements, Be, or Mg; $B$ are Group III or IV elements, and $C$ are Group VI or V elements.\n",
    "\n",
    "Following Dey *et al.*, we compute features based on the elemental properties of the $A$, $B$, and $C$ elements using all available elemental properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>composition</th>\n",
       "      <th>band_gap</th>\n",
       "      <th>AN_A</th>\n",
       "      <th>AN_B</th>\n",
       "      <th>AN_C</th>\n",
       "      <th>EN_A</th>\n",
       "      <th>EN_B</th>\n",
       "      <th>EN_C</th>\n",
       "      <th>MP_A</th>\n",
       "      <th>MP_B</th>\n",
       "      <th>MP_C</th>\n",
       "      <th>PR_A</th>\n",
       "      <th>PR_B</th>\n",
       "      <th>PR_C</th>\n",
       "      <th>VL_A</th>\n",
       "      <th>VL_B</th>\n",
       "      <th>VL_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CuAlS2</td>\n",
       "      <td>3.49</td>\n",
       "      <td>29.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1358.0</td>\n",
       "      <td>933.5</td>\n",
       "      <td>388.4</td>\n",
       "      <td>2.040</td>\n",
       "      <td>1.675</td>\n",
       "      <td>1.100</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CuAlSe2</td>\n",
       "      <td>2.67</td>\n",
       "      <td>29.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.54</td>\n",
       "      <td>1358.0</td>\n",
       "      <td>933.5</td>\n",
       "      <td>494.0</td>\n",
       "      <td>2.040</td>\n",
       "      <td>1.675</td>\n",
       "      <td>1.285</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CuAlTe2</td>\n",
       "      <td>2.06</td>\n",
       "      <td>29.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1358.0</td>\n",
       "      <td>933.5</td>\n",
       "      <td>722.7</td>\n",
       "      <td>2.040</td>\n",
       "      <td>1.675</td>\n",
       "      <td>1.670</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CuGaS2</td>\n",
       "      <td>2.43</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1358.0</td>\n",
       "      <td>302.9</td>\n",
       "      <td>388.4</td>\n",
       "      <td>2.040</td>\n",
       "      <td>1.695</td>\n",
       "      <td>1.100</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CuGaSe2</td>\n",
       "      <td>1.68</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2.54</td>\n",
       "      <td>1358.0</td>\n",
       "      <td>302.9</td>\n",
       "      <td>494.0</td>\n",
       "      <td>2.040</td>\n",
       "      <td>1.695</td>\n",
       "      <td>1.285</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CuGaTe2</td>\n",
       "      <td>1.12</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1358.0</td>\n",
       "      <td>302.9</td>\n",
       "      <td>722.7</td>\n",
       "      <td>2.040</td>\n",
       "      <td>1.695</td>\n",
       "      <td>1.670</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CuInS2</td>\n",
       "      <td>1.53</td>\n",
       "      <td>29.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1358.0</td>\n",
       "      <td>429.8</td>\n",
       "      <td>388.4</td>\n",
       "      <td>2.040</td>\n",
       "      <td>2.050</td>\n",
       "      <td>1.100</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CuInSe2</td>\n",
       "      <td>1.04</td>\n",
       "      <td>29.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.54</td>\n",
       "      <td>1358.0</td>\n",
       "      <td>429.8</td>\n",
       "      <td>494.0</td>\n",
       "      <td>2.040</td>\n",
       "      <td>2.050</td>\n",
       "      <td>1.285</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CuInTe2</td>\n",
       "      <td>1.06</td>\n",
       "      <td>29.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1358.0</td>\n",
       "      <td>429.8</td>\n",
       "      <td>722.7</td>\n",
       "      <td>2.040</td>\n",
       "      <td>2.050</td>\n",
       "      <td>1.670</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AgAlS2</td>\n",
       "      <td>3.13</td>\n",
       "      <td>47.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>933.5</td>\n",
       "      <td>388.4</td>\n",
       "      <td>2.375</td>\n",
       "      <td>1.675</td>\n",
       "      <td>1.100</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AgAlSe2</td>\n",
       "      <td>2.55</td>\n",
       "      <td>47.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.54</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>933.5</td>\n",
       "      <td>494.0</td>\n",
       "      <td>2.375</td>\n",
       "      <td>1.675</td>\n",
       "      <td>1.285</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AgAlTe2</td>\n",
       "      <td>2.27</td>\n",
       "      <td>47.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>933.5</td>\n",
       "      <td>722.7</td>\n",
       "      <td>2.375</td>\n",
       "      <td>1.675</td>\n",
       "      <td>1.670</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AgGaS2</td>\n",
       "      <td>2.64</td>\n",
       "      <td>47.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>302.9</td>\n",
       "      <td>388.4</td>\n",
       "      <td>2.375</td>\n",
       "      <td>1.695</td>\n",
       "      <td>1.100</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AgGaSe2</td>\n",
       "      <td>1.80</td>\n",
       "      <td>47.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2.54</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>302.9</td>\n",
       "      <td>494.0</td>\n",
       "      <td>2.375</td>\n",
       "      <td>1.695</td>\n",
       "      <td>1.285</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AgGaTe2</td>\n",
       "      <td>1.32</td>\n",
       "      <td>47.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>302.9</td>\n",
       "      <td>722.7</td>\n",
       "      <td>2.375</td>\n",
       "      <td>1.695</td>\n",
       "      <td>1.670</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AgInS2</td>\n",
       "      <td>1.87</td>\n",
       "      <td>47.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>429.8</td>\n",
       "      <td>388.4</td>\n",
       "      <td>2.375</td>\n",
       "      <td>2.050</td>\n",
       "      <td>1.100</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AgInSe2</td>\n",
       "      <td>1.24</td>\n",
       "      <td>47.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.54</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>429.8</td>\n",
       "      <td>494.0</td>\n",
       "      <td>2.375</td>\n",
       "      <td>2.050</td>\n",
       "      <td>1.285</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AgInTe2</td>\n",
       "      <td>0.95</td>\n",
       "      <td>47.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>429.8</td>\n",
       "      <td>722.7</td>\n",
       "      <td>2.375</td>\n",
       "      <td>2.050</td>\n",
       "      <td>1.670</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ZnSiP2</td>\n",
       "      <td>2.07</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.32</td>\n",
       "      <td>692.7</td>\n",
       "      <td>1687.0</td>\n",
       "      <td>317.3</td>\n",
       "      <td>1.880</td>\n",
       "      <td>1.420</td>\n",
       "      <td>1.240</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ZnSiAs2</td>\n",
       "      <td>1.74</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.27</td>\n",
       "      <td>692.7</td>\n",
       "      <td>1687.0</td>\n",
       "      <td>1089.0</td>\n",
       "      <td>1.880</td>\n",
       "      <td>1.420</td>\n",
       "      <td>1.415</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ZnGeP2</td>\n",
       "      <td>2.05</td>\n",
       "      <td>30.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.32</td>\n",
       "      <td>692.7</td>\n",
       "      <td>1211.0</td>\n",
       "      <td>317.3</td>\n",
       "      <td>1.880</td>\n",
       "      <td>1.560</td>\n",
       "      <td>1.240</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ZnGeAs2</td>\n",
       "      <td>1.15</td>\n",
       "      <td>30.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.27</td>\n",
       "      <td>692.7</td>\n",
       "      <td>1211.0</td>\n",
       "      <td>1089.0</td>\n",
       "      <td>1.880</td>\n",
       "      <td>1.560</td>\n",
       "      <td>1.415</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CdSiP2</td>\n",
       "      <td>2.33</td>\n",
       "      <td>48.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.32</td>\n",
       "      <td>594.3</td>\n",
       "      <td>1687.0</td>\n",
       "      <td>317.3</td>\n",
       "      <td>2.215</td>\n",
       "      <td>1.420</td>\n",
       "      <td>1.240</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>CdSiAs2</td>\n",
       "      <td>1.55</td>\n",
       "      <td>48.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.27</td>\n",
       "      <td>594.3</td>\n",
       "      <td>1687.0</td>\n",
       "      <td>1089.0</td>\n",
       "      <td>2.215</td>\n",
       "      <td>1.420</td>\n",
       "      <td>1.415</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>CdGeP2</td>\n",
       "      <td>1.72</td>\n",
       "      <td>48.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.32</td>\n",
       "      <td>594.3</td>\n",
       "      <td>1211.0</td>\n",
       "      <td>317.3</td>\n",
       "      <td>2.215</td>\n",
       "      <td>1.560</td>\n",
       "      <td>1.240</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>CdGeAs2</td>\n",
       "      <td>0.57</td>\n",
       "      <td>48.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.27</td>\n",
       "      <td>594.3</td>\n",
       "      <td>1211.0</td>\n",
       "      <td>1089.0</td>\n",
       "      <td>2.215</td>\n",
       "      <td>1.560</td>\n",
       "      <td>1.415</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>CdSnP2</td>\n",
       "      <td>1.17</td>\n",
       "      <td>48.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.88</td>\n",
       "      <td>2.32</td>\n",
       "      <td>594.3</td>\n",
       "      <td>505.1</td>\n",
       "      <td>317.3</td>\n",
       "      <td>2.215</td>\n",
       "      <td>1.880</td>\n",
       "      <td>1.240</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>CdSnAs2</td>\n",
       "      <td>0.26</td>\n",
       "      <td>48.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.88</td>\n",
       "      <td>2.27</td>\n",
       "      <td>594.3</td>\n",
       "      <td>505.1</td>\n",
       "      <td>1089.0</td>\n",
       "      <td>2.215</td>\n",
       "      <td>1.880</td>\n",
       "      <td>1.415</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   composition  band_gap  AN_A  AN_B  AN_C  EN_A  EN_B  EN_C    MP_A    MP_B  \\\n",
       "0       CuAlS2      3.49  29.0  13.0  16.0  1.08  1.64  2.65  1358.0   933.5   \n",
       "1      CuAlSe2      2.67  29.0  13.0  34.0  1.08  1.64  2.54  1358.0   933.5   \n",
       "2      CuAlTe2      2.06  29.0  13.0  52.0  1.08  1.64  2.38  1358.0   933.5   \n",
       "3       CuGaS2      2.43  29.0  31.0  16.0  1.08  1.70  2.65  1358.0   302.9   \n",
       "4      CuGaSe2      1.68  29.0  31.0  34.0  1.08  1.70  2.54  1358.0   302.9   \n",
       "5      CuGaTe2      1.12  29.0  31.0  52.0  1.08  1.70  2.38  1358.0   302.9   \n",
       "6       CuInS2      1.53  29.0  49.0  16.0  1.08  1.63  2.65  1358.0   429.8   \n",
       "7      CuInSe2      1.04  29.0  49.0  34.0  1.08  1.63  2.54  1358.0   429.8   \n",
       "8      CuInTe2      1.06  29.0  49.0  52.0  1.08  1.63  2.38  1358.0   429.8   \n",
       "9       AgAlS2      3.13  47.0  13.0  16.0  1.07  1.64  2.65  1235.0   933.5   \n",
       "10     AgAlSe2      2.55  47.0  13.0  34.0  1.07  1.64  2.54  1235.0   933.5   \n",
       "11     AgAlTe2      2.27  47.0  13.0  52.0  1.07  1.64  2.38  1235.0   933.5   \n",
       "12      AgGaS2      2.64  47.0  31.0  16.0  1.07  1.70  2.65  1235.0   302.9   \n",
       "13     AgGaSe2      1.80  47.0  31.0  34.0  1.07  1.70  2.54  1235.0   302.9   \n",
       "14     AgGaTe2      1.32  47.0  31.0  52.0  1.07  1.70  2.38  1235.0   302.9   \n",
       "15      AgInS2      1.87  47.0  49.0  16.0  1.07  1.63  2.65  1235.0   429.8   \n",
       "16     AgInSe2      1.24  47.0  49.0  34.0  1.07  1.63  2.54  1235.0   429.8   \n",
       "17     AgInTe2      0.95  47.0  49.0  52.0  1.07  1.63  2.38  1235.0   429.8   \n",
       "18      ZnSiP2      2.07  30.0  14.0  15.0  1.44  1.98  2.32   692.7  1687.0   \n",
       "19     ZnSiAs2      1.74  30.0  14.0  33.0  1.44  1.98  2.27   692.7  1687.0   \n",
       "20      ZnGeP2      2.05  30.0  32.0  15.0  1.44  1.99  2.32   692.7  1211.0   \n",
       "21     ZnGeAs2      1.15  30.0  32.0  33.0  1.44  1.99  2.27   692.7  1211.0   \n",
       "22      CdSiP2      2.33  48.0  14.0  15.0  1.40  1.98  2.32   594.3  1687.0   \n",
       "23     CdSiAs2      1.55  48.0  14.0  33.0  1.40  1.98  2.27   594.3  1687.0   \n",
       "24      CdGeP2      1.72  48.0  32.0  15.0  1.40  1.99  2.32   594.3  1211.0   \n",
       "25     CdGeAs2      0.57  48.0  32.0  33.0  1.40  1.99  2.27   594.3  1211.0   \n",
       "26      CdSnP2      1.17  48.0  50.0  15.0  1.40  1.88  2.32   594.3   505.1   \n",
       "27     CdSnAs2      0.26  48.0  50.0  33.0  1.40  1.88  2.27   594.3   505.1   \n",
       "\n",
       "      MP_C   PR_A   PR_B   PR_C  VL_A  VL_B  VL_C  \n",
       "0    388.4  2.040  1.675  1.100  11.0   3.0   6.0  \n",
       "1    494.0  2.040  1.675  1.285  11.0   3.0   6.0  \n",
       "2    722.7  2.040  1.675  1.670  11.0   3.0   6.0  \n",
       "3    388.4  2.040  1.695  1.100  11.0   3.0   6.0  \n",
       "4    494.0  2.040  1.695  1.285  11.0   3.0   6.0  \n",
       "5    722.7  2.040  1.695  1.670  11.0   3.0   6.0  \n",
       "6    388.4  2.040  2.050  1.100  11.0   3.0   6.0  \n",
       "7    494.0  2.040  2.050  1.285  11.0   3.0   6.0  \n",
       "8    722.7  2.040  2.050  1.670  11.0   3.0   6.0  \n",
       "9    388.4  2.375  1.675  1.100  11.0   3.0   6.0  \n",
       "10   494.0  2.375  1.675  1.285  11.0   3.0   6.0  \n",
       "11   722.7  2.375  1.675  1.670  11.0   3.0   6.0  \n",
       "12   388.4  2.375  1.695  1.100  11.0   3.0   6.0  \n",
       "13   494.0  2.375  1.695  1.285  11.0   3.0   6.0  \n",
       "14   722.7  2.375  1.695  1.670  11.0   3.0   6.0  \n",
       "15   388.4  2.375  2.050  1.100  11.0   3.0   6.0  \n",
       "16   494.0  2.375  2.050  1.285  11.0   3.0   6.0  \n",
       "17   722.7  2.375  2.050  1.670  11.0   3.0   6.0  \n",
       "18   317.3  1.880  1.420  1.240  12.0   4.0   5.0  \n",
       "19  1089.0  1.880  1.420  1.415  12.0   4.0   5.0  \n",
       "20   317.3  1.880  1.560  1.240  12.0   4.0   5.0  \n",
       "21  1089.0  1.880  1.560  1.415  12.0   4.0   5.0  \n",
       "22   317.3  2.215  1.420  1.240  12.0   4.0   5.0  \n",
       "23  1089.0  2.215  1.420  1.415  12.0   4.0   5.0  \n",
       "24   317.3  2.215  1.560  1.240  12.0   4.0   5.0  \n",
       "25  1089.0  2.215  1.560  1.415  12.0   4.0   5.0  \n",
       "26   317.3  2.215  1.880  1.240  12.0   4.0   5.0  \n",
       "27  1089.0  2.215  1.880  1.415  12.0   4.0   5.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes = np.zeros((len(training_set), 15))\n",
    "\n",
    "elem_re = re.compile('[A-Z][a-z]?')\n",
    "\n",
    "# Compute the features\n",
    "for i in range(len(training_set)):\n",
    "    elements = elem_re.findall(training_set[\"composition\"][i])\n",
    "    for j in range(len(elements)):\n",
    "        attributes[i,5*j:5*(j+1)] = elem_properties[elements[j]]\n",
    "\n",
    "# Add them to dataframe\n",
    "attr_labels = [\"EN\", \"AN\", \"MP\", \"PR\", \"VL\"]\n",
    "atom_labels = [\"A\", \"B\", \"C\"]\n",
    "\n",
    "labels = []\n",
    "for i in range(len(atom_labels)):\n",
    "    for j in range(len(attr_labels)):\n",
    "        labels.append(\"%s_%s\"%(attr_labels[j], atom_labels[i]))\n",
    "\n",
    "training_set = training_set.assign(\n",
    "    **dict(zip(labels, [attributes[:, i] for i in range(np.shape(attributes)[1])])))\n",
    "\n",
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Subset Selection\n",
    "Here, we use the subset selection technique employed by Dey *et al.* to determine which group of features, out of all 15 used to describe our training set, lead to the most predictive linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_size = len(labels) if True else 7 # Set to True to run all combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possible subsets: 32767\n"
     ]
    }
   ],
   "source": [
    "all_subsets = sum([list(itertools.combinations(labels, s)) for s in range(1, max_size+1)], [])\n",
    "print 'Number of possible subsets:', len(all_subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Run a leave-on-out cross-validation test on every subset of attributes. We use Ordinary Least Squares, but it should work with any of the regression methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate(training_set, model, labels=labels):\n",
    "    \"\"\"Compute the mean squared error for a leave-one-out cross-validation test\n",
    "    \n",
    "    Args: training_set (DataFrame) - Data used for cr\"\"\"\n",
    "    errors = cross_val_score(model, training_set[labels], training_set['band_gap'], n_jobs=1,\n",
    "                             scoring='neg_mean_squared_error', cv=LeaveOneOut())\n",
    "    mscve = np.mean(np.abs(errors))\n",
    "    return mscve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test every subset of a given size\n",
    "def test_subset(training_set, subset, method=LinearRegression()):\n",
    "    \"\"\"Test out a certain subset of attributes\n",
    "    \n",
    "    Args: training_set - Data used for fitting\n",
    "          subset - Subset of attributes to use as inputs\n",
    "\n",
    "    Returns: Mean squared error in LOOCV\"\"\"\n",
    "\n",
    "    #Crossvalidate\n",
    "    return cross_validate(training_set, method, list(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32767/32767 - EN_A,AN_A,MP_A,PR_A,VL_A,EN_B,AN_B,MP_B,PR_B,VL_B,EN_C,AN_C,MP_C,PR_C,VL_C                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n"
     ]
    }
   ],
   "source": [
    "mscve = []\n",
    "for i,subset in enumerate(all_subsets):\n",
    "    print \"\\r %d/%d - %s\"%(i+1, len(all_subsets), ','.join(subset)),\n",
    "    mscve.append(test_subset(training_set, subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attr_selection_results = pd.DataFrame({'subset':all_subsets, 'mscve':mscve})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot error metrics vs number of attributes\n",
    "Determine which number of descriptors leads to the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_selection_results['size'] = attr_selection_results['subset'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attr_selection_by_size = pd.DataFrame(dict([(i, {'size': i, 'min': g['mscve'].min(), 'mean': g['mscve'].mean(), \n",
    "                                                 'Below 0.06':float(sum(g['mscve'] < 0.06)) / len(g)})\n",
    "                                            for i,g in attr_selection_results.groupby('size')])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0xdafa400>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAEKCAYAAABdWiGrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvDX4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbd95ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.semilogy(attr_selection_by_size['size'], attr_selection_by_size['min'])\n",
    "\n",
    "ax.set_xlabel('Number of attributes')\n",
    "ax.set_ylabel('Best MSCVE (eV$^2$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given enough features, the model tends to overfit. Too few, and the model is underfit. The optimimum is somewhere around 5 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Top Attributes\n",
    "Print out the top 5 (see Table 4 of Dey *et al*). Many choices of models are equivalent because there are limited numbers of chemistries (i.e., $I-III-VI_2$ and $II-IV-V_2$), which makes the valence attributes (e.g., `VL_A`) collinear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get nearly-equivalent models (tol=1e-8)\n",
    "attr_selection_results['score'] = attr_selection_results['mscve'].apply(lambda x: int(x * 1e8))\n",
    "attr_selection_results.sort_values(['score', 'size'], ascending=True, inplace=True)\n",
    "attr_selection_results.drop_duplicates('score', keep='first', inplace=True)\n",
    "attr_selection_results.drop('score', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attr_selection_results['subset'] = attr_selection_results['subset'].apply(lambda x: sorted(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mscve</th>\n",
       "      <th>subset</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>0.051031</td>\n",
       "      <td>[AN_B, AN_C, PR_B, PR_C, VL_A]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4504</th>\n",
       "      <td>0.051474</td>\n",
       "      <td>[AN_B, AN_C, EN_B, PR_C, VL_A]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>0.052467</td>\n",
       "      <td>[AN_B, AN_C, PR_C, VL_A]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4757</th>\n",
       "      <td>0.052719</td>\n",
       "      <td>[AN_C, EN_B, MP_B, PR_B, PR_C]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4591</th>\n",
       "      <td>0.052871</td>\n",
       "      <td>[AN_B, AN_C, EN_C, PR_B, VL_A]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mscve                          subset  size\n",
       "4596  0.051031  [AN_B, AN_C, PR_B, PR_C, VL_A]     5\n",
       "4504  0.051474  [AN_B, AN_C, EN_B, PR_C, VL_A]     5\n",
       "1669  0.052467        [AN_B, AN_C, PR_C, VL_A]     4\n",
       "4757  0.052719  [AN_C, EN_B, MP_B, PR_B, PR_C]     5\n",
       "4591  0.052871  [AN_B, AN_C, EN_C, PR_B, VL_A]     5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_selection_results.sort_values(['mscve', 'size'], ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best subset: ['AN_B', 'AN_C', 'PR_B', 'PR_C', 'VL_A']\n"
     ]
    }
   ],
   "source": [
    "best_subset = list(attr_selection_results.sort_values('mscve', ascending=True).iloc[0]['subset'])\n",
    "print 'Best subset:', best_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features in our best subsets and the scores agree with the findings of Dey *et al* (Table 4 of their paper). The only exception is minor: our top score is `0.0510`, where the value in Table 4 is `0.0501`. Given the excellent agreement otherwise, we assume this is a typo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Regression Methods\n",
    "In this section, we evaluate the performance of different machine learning models. As the original code used by Dey *et al* is not available, we attempt to implement these methods using [scikit-learn](http://scikit-learn.org/stable/) where possible.\n",
    "\n",
    "We first determine the optimal hyper-parameters for each model. e then compare their fitness on the original training set and in cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a general fitting function. It runs a fitting procedure using the provided regression method implemented in sci-kit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_parameters(training_set, method, labels=labels, **kwargs):\n",
    "    \"\"\"Determine the optimal set of parameters for an algorithm\n",
    "    \n",
    "    Uses leave-one-out cross-validation\n",
    "    \n",
    "    Args:     training_set (Dataframe): Data uses for tuning\n",
    "              labels (list): Columns of training_set to use as inputs\n",
    "              method (sklearn regressor): Model to be turned\n",
    "    Kwargs: parameters to be tuned, and their search ranges\n",
    "    Returns: Copy of the model with the parameters tuned\"\"\"\n",
    "    \n",
    "    return GridSearchCV(method, kwargs, cv=LeaveOneOut(), scoring='neg_mean_squared_error')\\\n",
    "            .fit(training_set[labels], training_set['band_gap']).best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_data(training_set, model, labels=labels, test_set=None):\n",
    "    \"\"\" General function to fit a regression model and make a prediction.\n",
    "    \n",
    "        Args:    training_set (DataFrame): Inputs and outputs to the model\n",
    "                 labels (list): Columns of training_set to use as inputs\n",
    "                 test_set (DataFrame): Data to be evaluated (if None, run training_set)\n",
    "                 model (sklearn model): Model to be tested\n",
    "        Returns: updated DataFrame (ndarray)\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(training_set[labels], training_set['band_gap'])\n",
    "    \n",
    "    # Run the prediction\n",
    "    if test_set is None:\n",
    "        test_set = training_set\n",
    "    return model.predict(test_set[labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Ordinary Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ols_model = optimize_parameters(training_set, LinearRegression(), fit_intercept=[True, False], normalize=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy_X': True, 'fit_intercept': True, 'n_jobs': 1, 'normalize': True}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_set['band_gap ols'] = fit_data(training_set, ols_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1b) Ordinary Linear Regression, on a small subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set['band_gap ols,selected'] = fit_data(training_set, ols_model, best_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Lasso, implemented with LARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.016e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.016e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=9.257e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=7.598e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=9.457e-04, previous alpha=6.090e-04, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.868e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.868e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=1.856e-03, previous alpha=1.667e-03, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.552e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.536e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.536e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=8.531e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=6.583e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.336e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.130e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.973e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.861e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.281e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.557e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.557e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=1.490e-03, previous alpha=1.425e-03, with an active set of 6 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.641e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.037e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.720e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.058e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=9.243e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=9.243e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.284e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.674e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.647e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=1.623e-03, previous alpha=1.584e-03, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.294e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.303e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.303e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.981e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.981e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.665e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.582e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=9.181e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.817e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=6.677e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=6.021e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.600e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.630e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.630e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=1.586e-03, previous alpha=1.484e-03, with an active set of 6 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.939e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.939e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.724e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.724e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.801e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=5.758e-04, previous alpha=3.528e-04, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=6.043e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=6.015e-04, previous alpha=3.659e-04, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.942e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=5.924e-04, previous alpha=3.641e-04, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=6.491e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=6.477e-04, previous alpha=3.987e-04, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.638e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.638e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.879e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.879e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.814e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.814e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=5.798e-04, previous alpha=3.547e-04, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.667e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.667e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Logan\\Miniconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.169e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lasso_model = optimize_parameters(training_set, LassoLars(max_iter=13), fit_intercept=[True, False],\n",
    "                           alpha=np.logspace(-14,-6,11), normalize=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1e-14,\n",
       " 'copy_X': True,\n",
       " 'eps': 2.2204460492503131e-16,\n",
       " 'fit_intercept': True,\n",
       " 'fit_path': True,\n",
       " 'max_iter': 13,\n",
       " 'normalize': True,\n",
       " 'positive': False,\n",
       " 'precompute': 'auto',\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set['band_gap lasso'] = fit_data(training_set, model=lasso_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Partial Least Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pls_model = optimize_parameters(training_set, PLSRegression(), n_components=range(1, 10), scale=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy': True, 'max_iter': 500, 'n_components': 8, 'scale': True, 'tol': 1e-06}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pls_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set['band_gap pls'] = fit_data(training_set, pls_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Sparse Partial Least Squares\n",
    "\n",
    "_LW 7Jul17_: The only implementation of SPLS method I found is in R ([docs](https://cran.r-project.org/web/packages/spls/spls.pdf)). Given that it has the worst performance in Dey *et al*, we elect to not test it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predicted vs measured results\n",
    "Compare the band gaps predicted using each regression method vs. band gaps determined experimentally. See Figure 3 of Dey _et al_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAAEYCAYAAACHjumMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nP4Ug2CwGHTXrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbc5eda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#band_gaps_predicted = np.zeros(np.shape(band_gaps))\n",
    "\n",
    "plt.scatter(training_set['band_gap'], training_set['band_gap ols,selected'], color='k', label=\"OLS\")\n",
    "plt.scatter(training_set['band_gap'], training_set['band_gap lasso'], color='g', label=\"LASSO\")\n",
    "plt.scatter(training_set['band_gap'], training_set['band_gap pls'], color='r', label=\"PLS\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Measured Band Gap (eV)\")\n",
    "plt.ylabel(\"Predicted Band Gap (eV)\")\n",
    "plt.axis([0,4,0,4])\n",
    "plt.plot((0,5), (0,5))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(4, 4)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('dey-2014.png', dpi=320)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Calculate Mean Square Cross-Validation Error\n",
    "\n",
    "Crossvalidate all the regression methods using Leave-One-Out method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean-Squared Cross-Validation Errors \n",
      "\n",
      "OLS w/ best subset: 0.051031\n",
      "LASSO: 0.059833\n",
      "PLS: 0.064903\n",
      "OLS w/ all features: 0.072294\n"
     ]
    }
   ],
   "source": [
    "olss_mscve = cross_validate(training_set, ols_model, labels=best_subset)\n",
    "lasso_mscve = cross_validate(training_set, lasso_model)\n",
    "pls_mscve = cross_validate(training_set, pls_model)\n",
    "ols_mscve = cross_validate(training_set, ols_model)\n",
    "\n",
    "print \"Mean-Squared Cross-Validation Errors \\n\"\n",
    "print \"OLS w/ best subset: %f\"%olss_mscve\n",
    "print \"LASSO: %f\"%lasso_mscve\n",
    "print \"PLS: %f\"%pls_mscve\n",
    "print \"OLS w/ all features: %f\"%ols_mscve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not exactly the same, our results are in general agreement with Dey. The OLS with feature selection is the best algorithm in LOOCV, followed by LASSO, and PLS. We note that our recreation is limited in that we did not use *sparse* PLS or the same LASSO implementation. Those differences in algorithm may account for the ~20% deviation in our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
